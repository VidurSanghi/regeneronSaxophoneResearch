# -*- coding: utf-8 -*-

"""

import os
import librosa
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers, models

def extract_audio_features(audio_file):
    y, sr = librosa.load(audio_file)

    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)[0]
    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
    spectral_flatness = librosa.feature.spectral_flatness(y=y)[0]
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    mfccs = librosa.feature.mfcc(y=y, sr=sr)
    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
    rms = librosa.feature.rms(y=y)[0]

    return {
        'Zero Crossing Rate': zero_crossing_rate,
        'Spectral Centroid': spectral_centroid,
        'Spectral Bandwidth': spectral_bandwidth,
        'Spectral Contrast': spectral_contrast,
        'Spectral Flatness': spectral_flatness,
        'Chroma': chroma,
        'Mfccs': mfccs,
        'Spectral Rolloff': spectral_rolloff,
        'Tonnetz': tonnetz,
        'Rms': rms
    }

# Directory containing audio files - CHANGE THIS!
audio_dir = 'audioDataFolder'

performer_labels = []
audio_features = []

# Iterate through the audio files in the directory
for file in os.listdir(audio_dir):
    if file.endswith(".wav"):
        file_path = os.path.join(audio_dir, file)

        # Extract MFCCs for each audio file
        #mfccs = extract_mfcc(file_path)
        #audio_features.append(mfccs)

        # Extract performer labels from the file name
        performer_label = file.split('_')[2]
        performer_labels.append(performer_label)

def get_statistics(aggregated_audio_features, performer_labels):

  from scipy.stats import pearsonr
  import matplotlib.pyplot as plt

  audio_features = aggregated_audio_features

  # Convert lists to NumPy arrays
  audio_features = np.array(audio_features)
  performer_labels = np.array(performer_labels)

  # Encode the performer labels (Professional and Beginner) to numerical values
  label_encoder = LabelEncoder()
  performer_labels_encoded = label_encoder.fit_transform(performer_labels)
  if(len(audio_features.shape) <= 1):
    correlations = [pearsonr(audio_features, performer_labels_encoded)[0]]
    p_values = [pearsonr(audio_features, performer_labels_encoded)[1]]
  else:
    # Calculate the Pearson correlation coefficient for each feature
    correlations = [pearsonr(audio_features[:, i], performer_labels_encoded)[0] for i in range(audio_features.shape[1])]
    p_values = [pearsonr(audio_features[:, i], performer_labels_encoded)[1] for i in range(audio_features.shape[1])]


  print("P values", p_values)
  print("correlations", correlations)

  indices_for_statistically_significant_features = [i for i in range(len(p_values)) if p_values[i] < 0.05]
  print(indices_for_statistically_significant_features)

  for index in indices_for_statistically_significant_features:


    # Plot histograms to visualize the distribution of the identified feature for both groups
    current_feature = audio_features[:, index]

    # Extract indices of Professional and Beginner performers
    professional_indices = performer_labels_encoded == label_encoder.transform(['Professional'])[0]
    beginner_indices = performer_labels_encoded == label_encoder.transform(['Beginner'])[0]

    # Plot histograms for the identified feature
    plt.figure(figsize=(10, 6))
    plt.hist(current_feature[professional_indices], bins=20, alpha=0.7, label='Professional', color='blue')
    plt.hist(current_feature[beginner_indices], bins=20, alpha=0.7, label='Beginner', color='orange')
    plt.xlabel(f'Current Feature Index {index} Values')
    plt.ylabel('Frequency')
    plt.title(f'Distribution of Current Feature Index {index} for Professional and Beginner Performers')
    plt.legend()
    plt.grid(True)
    plt.show()

file_paths = os.listdir(audio_dir)
file_paths[-1]

from librosa.feature import spectral_flatness
from librosa.feature.spectral import spectral_contrast

# Initialize
file_paths = os.listdir(audio_dir)
aggregate_spectral_centroid_features = []
aggregate_spectral_bandwidth_features = []
aggregate_spectral_flatness_features = []
aggregate_spectral_contrast_features = []
aggregate_chroma_features = []
aggregate_spectral_rolloff_features = []
aggregate_tonnetz_features = []
aggregate_rms_features = []
aggregate_mfccs_features = []

for i, file_path in enumerate(file_paths):
  print(i)
  file_path = os.path.join(audio_dir, file_path)
  features = extract_audio_features(file_path)

  spectral_centroid = features['Spectral Centroid']
  aggregate_spectral_centroid = np.mean(spectral_centroid)
  aggregate_spectral_centroid_features.append(aggregate_spectral_centroid)


  spectral_bandwidth = features['Spectral Bandwidth']
  aggregate_spectral_bandwidth = np.mean(spectral_centroid)
  aggregate_spectral_bandwidth_features.append(aggregate_spectral_bandwidth)

  spectral_flatness = features['Spectral Flatness']
  aggregate_spectral_flatness =  np.mean(spectral_flatness)
  aggregate_spectral_flatness_features.append(aggregate_spectral_flatness)


  spectral_contrast = features['Spectral Contrast']
  aggregate_spectral_contrast =  np.mean(spectral_contrast, axis = 1)
  aggregate_spectral_contrast_features.append(aggregate_spectral_contrast)


  chroma = features['Chroma']
  aggregate_chroma =  np.mean(chroma, axis = 1)
  aggregate_chroma_features.append(aggregate_chroma)

  spectral_rolloff = features['Spectral Rolloff']
  aggregate_spectral_rolloff=  np.mean(spectral_rolloff)
  aggregate_spectral_rolloff_features.append(aggregate_spectral_rolloff)

  tonnetz = features['Tonnetz']
  aggregate_tonnetz =  np.mean(tonnetz, axis = 1)
  aggregate_tonnetz_features.append(aggregate_tonnetz)

  mfccs = features['Mfccs']
  aggregate_mfccs =  np.mean(mfccs, axis = 1)
  aggregate_mfccs_features.append(aggregate_mfccs)

  rms = features['Rms']
  aggregate_rms =  np.mean(rms)
  aggregate_rms_features.append(aggregate_rms)

get_statistics(aggregate_spectral_centroid_features, performer_labels)

get_statistics(aggregate_spectral_bandwidth_features, performer_labels)

aggregate_spectral_flatness_features_reshaped = np.reshape(aggregate_spectral_flatness_features, (-1, 1))
get_statistics(aggregate_spectral_flatness_features_reshaped, performer_labels)

get_statistics(aggregate_spectral_contrast_features, performer_labels)

get_statistics(aggregate_chroma_features, performer_labels)

aggregate_spectral_rolloff_features_reshaped = np.reshape(aggregate_spectral_rolloff_features, (-1, 1))
get_statistics(aggregate_spectral_rolloff_features_reshaped, performer_labels)

get_statistics(aggregate_tonnetz_features, performer_labels)

get_statistics(aggregate_mfccs_features, performer_labels)

aggregate_rms_features_reshaped = np.reshape(aggregate_rms_features, (-1, 1))
get_statistics(aggregate_rms_features_reshaped, performer_labels)
